{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "id": "kJYsx-Nfc1oA",
        "outputId": "1e8f2413-6f98-4a13-f45c-b4b70b85d9ba"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "Module langchain_community.embeddings not found. Please install langchain-community to access this module. You can install it using `pip install -U langchain-community`",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain/_api/module_import.py\u001b[0m in \u001b[0;36mimport_by_name\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                 \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_community'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3291763868.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_splitter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRecursiveCharacterTextSplitter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHuggingFaceEmbeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorstores\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFAISS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain/embeddings/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;34m\"\"\"Look up attributes dynamically.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_import_attribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain/_api/module_import.py\u001b[0m in \u001b[0;36mimport_by_name\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     76\u001b[0m                         \u001b[0;34m\"You can install it using `pip install -U langchain-community`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                     )\n\u001b[0;32m---> 78\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: Module langchain_community.embeddings not found. Please install langchain-community to access this module. You can install it using `pip install -U langchain-community`",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "!pip install transformers torch gradio pymupdf langchain sentence-transformers faiss-cpu --quiet\n",
        "\n",
        "import gradio as gr\n",
        "import fitz  # PyMuPDF\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# ðŸ”¹ Load IBM Granite model\n",
        "model_id = \"ibm-granite/granite-3.3-2b-instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ðŸ”¹ PDF text extraction\n",
        "def extract_text(pdf_file):\n",
        "    doc = fitz.open(pdf_file)\n",
        "    return \"\\n\".join(page.get_text() for page in doc)\n",
        "\n",
        "# ðŸ”¹ Build vector store\n",
        "def build_vectorstore(texts):\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    chunks = []\n",
        "    for text in texts:\n",
        "        chunks.extend(splitter.split_text(text))\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    return FAISS.from_texts(chunks, embeddings)\n",
        "\n",
        "# ðŸ”¹ Ask question using IBM Granite\n",
        "chat_history = []\n",
        "\n",
        "def ask_question(pdfs, question):\n",
        "    texts = [extract_text(pdf) for pdf in pdfs]\n",
        "    vectorstore = build_vectorstore(texts)\n",
        "    docs = vectorstore.similarity_search(question, k=3)\n",
        "    context = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": f\"\"\"You are StudyMate, an academic assistant. Use the context below to answer the question.\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\\nAnswer:\"\"\"}\n",
        "    ]\n",
        "    inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(**inputs, max_new_tokens=300)\n",
        "    answer = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]).strip()\n",
        "\n",
        "    chat_history.append((question, answer))\n",
        "    return answer, chat_history\n",
        "\n",
        "# ðŸ”¹ Gradio UI\n",
        "with gr.Blocks(theme=gr.themes.Soft(primary_hue=\"blue\")) as demo:\n",
        "    gr.Markdown(\"## ðŸ“˜ StudyMate\\nYour AI-powered academic assistant\")\n",
        "\n",
        "    with gr.Row():\n",
        "        pdf_input = gr.File(label=\"Upload PDFs\", file_types=[\".pdf\"], file_count=\"multiple\")\n",
        "        question_input = gr.Textbox(label=\"Ask a question\", placeholder=\"e.g. What is the main idea of Chapter 3?\")\n",
        "\n",
        "    with gr.Row():\n",
        "        submit_btn = gr.Button(\"Get Answer\", variant=\"primary\")\n",
        "        answer_output = gr.Textbox(label=\"Answer\", lines=10, interactive=False)\n",
        "\n",
        "    chatbox = gr.Chatbot(label=\"Chat History\")\n",
        "\n",
        "    def handle_query(pdfs, question):\n",
        "        answer, history = ask_question(pdfs, question)\n",
        "        return answer, history\n",
        "\n",
        "    submit_btn.click(fn=handle_query, inputs=[pdf_input, question_input], outputs=[answer_output, chatbox])\n",
        "\n",
        "demo.launch(share=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers torch gradio pymupdf langchain sentence-transformers faiss-cpu --quiet\n",
        "\n",
        "import gradio as gr\n",
        "import fitz  # PyMuPDF\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# ðŸ”¹ Load IBM Granite model\n",
        "model_id = \"ibm-granite/granite-3.3-2b-instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# ðŸ”¹ PDF text extraction\n",
        "def extract_text(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    return \"\\n\".join(page.get_text() for page in doc)\n",
        "\n",
        "# ðŸ”¹ Build vector store\n",
        "def build_vectorstore(texts):\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    chunks = []\n",
        "    for text in texts:\n",
        "        chunks.extend(splitter.split_text(text))\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    return FAISS.from_texts(chunks, embeddings)\n",
        "\n",
        "# ðŸ”¹ Ask question using IBM Granite\n",
        "chat_history = []\n",
        "\n",
        "def ask_question(pdfs, question):\n",
        "    texts = [extract_text(pdf) for pdf in pdfs]\n",
        "    vectorstore = build_vectorstore(texts)\n",
        "    docs = vectorstore.similarity_search(question, k=3)\n",
        "    context = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "    prompt = f\"\"\"You are StudyMate, an academic assistant.\n",
        "Use the context below to answer the question.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Answer:\"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(**inputs, max_new_tokens=300)\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True).replace(prompt, \"\").strip()\n",
        "\n",
        "    chat_history.append((question, answer))\n",
        "    return answer, chat_history\n",
        "\n",
        "# ðŸ”¹ Gradio UI\n",
        "with gr.Blocks(theme=gr.themes.Soft(primary_hue=\"blue\")) as demo:\n",
        "    gr.Markdown(\"## ðŸ“˜ StudyMate\\nYour AI-powered academic assistant\")\n",
        "\n",
        "    with gr.Row():\n",
        "        pdf_input = gr.File(label=\"Upload PDFs\", file_types=[\".pdf\"], file_count=\"multiple\")\n",
        "        question_input = gr.Textbox(label=\"Ask a question\", placeholder=\"e.g. What is the main idea of Chapter 3?\")\n",
        "\n",
        "    with gr.Row():\n",
        "        submit_btn = gr.Button(\"Get Answer\", variant=\"primary\")\n",
        "        answer_output = gr.Textbox(label=\"Answer\", lines=10, interactive=False)\n",
        "\n",
        "    chatbox = gr.Chatbot(label=\"Chat History\")\n",
        "\n",
        "    def handle_query(pdfs, question):\n",
        "        if not pdfs or not question.strip():\n",
        "            return \"Please upload PDFs and ask a question.\", chat_history\n",
        "        answer, history = ask_question(pdfs, question)\n",
        "        return answer, history\n",
        "\n",
        "    submit_btn.click(fn=handle_query, inputs=[pdf_input, question_input], outputs=[answer_output, chatbox])\n",
        "\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "Em7ElyeTiy1j",
        "outputId": "08742299-726e-4a86-c350-575ccafd2072"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-3382005982.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-3382005982.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pip install transformers torch gradio pymupdf langchain sentence-transformers faiss-cpu --quiet\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}